===============================================================================
                    OBSIDIAN GRAPH RAG SYSTEM ARCHITECTURE
                           Technical Documentation
===============================================================================

OVERVIEW
========

This system implements Hybrid Graph RAG - a sophisticated approach that combines:
1. Traditional Vector RAG (semantic similarity search)
2. Knowledge Graph Traversal (relationship-based expansion)  
3. Sequential Reasoning (multi-step thinking for complex queries)

CORE ARCHITECTURE COMPONENTS
============================

Phase 1: Data Ingestion & Processing
------------------------------------
VaultScanner → MarkdownParser → LinkExtractor → Document Storage

Components:
- VaultScanner: Recursively scans Obsidian vault for .md files
- MarkdownParser: Extracts content, frontmatter, tags, and metadata
- LinkExtractor: Identifies [[wikilinks]], ![[embeds]], and #tags
- Creates Document objects with full structural information

Phase 2: Knowledge Graph Construction
------------------------------------
Documents → NetworkX Graph → Community Detection → Graph Storage

Structure:
- Nodes = Individual notes (documents)
- Edges = [[wikilinks]] between notes  
- Weights = Connection strength based on link frequency
- Communities = Clusters of related topics (Louvain algorithm)

Phase 3: Vector Embedding Generation
-----------------------------------
Documents → OpenAI Embeddings → Vector Storage → Semantic Search Index

Process:
- Each document → 1536-dimensional vector (text-embedding-3-small)
- Semantic similarity becomes mathematically computable
- Batch processing (20 docs at a time) for efficiency
- Caching to avoid regenerating expensive embeddings

Phase 4: Hybrid Retrieval (Graph RAG Magic)
-------------------------------------------
Query → Vector Search → Graph Expansion → Context Ranking → LLM Generation

DETAILED RETRIEVAL ALGORITHM
============================

Traditional RAG:
    Query → Find similar documents → Send to LLM → Get answer

Graph RAG Approach:
    def retrieve_context(query, documents, knowledge_graph, embedding_manager):
        # Step 1: Semantic search (like traditional RAG)
        semantic_results = embedding_manager.search_similar_documents(query, top_k=10)
        
        # Step 2: Graph expansion (the magic part)
        expanded_doc_ids = set()
        for doc_id, score in semantic_results:
            expanded_doc_ids.add(doc_id)  # Add original matches
            
            # Add connected documents (1-hop)
            neighbors = knowledge_graph.neighbors(doc_id)
            for neighbor in neighbors[:5]:
                expanded_doc_ids.add(neighbor)
                
                # Add connected documents (2-hop)
                second_hop = knowledge_graph.neighbors(neighbor)
                for second_neighbor in second_hop[:2]:
                    expanded_doc_ids.add(second_neighbor)
        
        # Step 3: Rank by semantic relevance
        return sorted_context_documents

Why This Works:
- Semantic matches find topically relevant content
- Graph traversal adds explicitly connected context
- Multi-hop expansion discovers related concepts 2-3 steps away
- Ranking ensures most relevant content appears first

ADVANCED FEATURES
=================

1. Sequential Thinking Orchestrator
----------------------------------
For complex queries like: "How do my notes on machine learning relate to my productivity systems?"

Process:
1. Decomposes → "Find ML notes" + "Find productivity notes" + "Analyze connections"
2. Retrieves → Relevant docs for each step
3. Reasons → Step-by-step analysis with context
4. Synthesizes → Comprehensive answer with citations

Implementation:
    class SequentialThinkingOrchestrator:
        async def run_sequential_thinking(query, retrieval_function):
            # 1. Decompose complex query into steps
            # 2. Execute retrieval for each step
            # 3. Reason through each step iteratively
            # 4. Synthesize final comprehensive answer

2. Intelligent Caching System
----------------------------
Benefits:
- First run: 2-10 minutes (processes entire vault)
- Subsequent runs: 5-15 seconds (loads from cache)
- Incremental updates: Only processes new/changed files

Implementation:
    class DataPersistenceManager:
        def detect_changes(vault_path):
            # Incremental processing - only updates changed files
            # Preserves expensive embeddings and graph structure
            # Smart cache invalidation

3. Multiple Interface Options
----------------------------
start_chat.py provides:
- Web Interface (Flask-based, modern UI)
- CLI Interface (Terminal-based chat)
- 3D Visualization (via graph3d_launcher.py)

DATA STRUCTURES & STORAGE
=========================

Core Data Types:
    @dataclass
    class Document:
        id: str              # Unique hash
        title: str           # Note title
        content: str         # Full text
        wikilinks: Set[str]  # [[links]] to other notes
        embedding: ndarray   # 1536-dim vector
        tags: Set[str]       # #hashtags

    @dataclass  
    class GraphRAGConfig:
        top_k_vector: int = 10    # How many semantic matches
        top_k_graph: int = 5      # How many graph neighbors
        max_graph_hops: int = 2   # How far to traverse

Storage Architecture:
    cache/processed_data/
    ├── documents.pkl        # All parsed documents
    ├── knowledge_graph.gpickle  # NetworkX graph structure
    ├── embeddings.pkl       # Vector embeddings
    └── metadata.json        # Processing timestamps

QUERY PROCESSING EXAMPLE
========================

User Query: "What did I write about attention mechanisms?"

Step 1: Vector Search
    query_embedding = openai.embed("attention mechanisms")
    semantic_matches = cosine_similarity(query_embedding, all_doc_embeddings)
    Returns: ["Transformer Architecture.md", "Deep Learning Fundamentals.md"]

Step 2: Graph Expansion
    # Start with semantic matches
    expanded_docs = {"Transformer Architecture.md", "Deep Learning Fundamentals.md"}

    # Add 1-hop neighbors (documents linked via [[wikilinks]])
    for doc in semantic_matches:
        neighbors = graph.neighbors(doc)
        expanded_docs.update(neighbors)
    Adds: ["Neural Networks.md", "Attention Paper Notes.md", "BERT Implementation.md"]

    # Add 2-hop neighbors
    for neighbor in neighbors:
        second_hop = graph.neighbors(neighbor)
        expanded_docs.update(second_hop[:2])
    Adds: ["NLP Applications.md", "Computer Vision.md"]

Step 3: Context Assembly
    context = format_context_for_llm(expanded_docs)
    Creates: 8,000-token context with all relevant documents

Step 4: LLM Generation
    messages = [
        {"role": "system", "content": "Answer based on provided context..."},
        {"role": "user", "content": f"Context: {context}\n\nQuestion: {query}"}
    ]
    response = openai.chat.completions.create(messages=messages)

PERFORMANCE OPTIMIZATIONS
=========================

1. Batch Processing
- Embeddings: Process 20 documents per API call
- Token limiting: Truncate long documents to stay under API limits
- Error handling: Fallback to individual processing if batch fails

2. Smart Caching
- File change detection: Only reprocess modified files
- Embedding persistence: Expensive vectors saved to disk
- Graph persistence: NetworkX structure cached as .gpickle

3. Memory Management
- Lazy loading: Load only needed data structures
- Streaming processing: Handle large vaults without memory issues
- Garbage collection: Clean up unused embeddings

CONFIGURATION & CUSTOMIZATION
=============================

Environment Variables:
    # Retrieval tuning
    TOP_K_VECTOR=10        # Semantic search results
    TOP_K_GRAPH=5          # Graph expansion per document
    MAX_TOKENS=2000        # Response length

    # Model selection  
    EMBEDDING_MODEL=text-embedding-3-small
    OPENAI_MODEL=gpt-4o

    # Performance tuning
    CACHE_DIR=./cache
    LOG_LEVEL=INFO

KEY SYSTEM CLASSES
==================

1. ObsidianGraphRAG (Main Orchestrator)
   - Coordinates all system components
   - Manages initialization and data flow
   - Provides main API interface

2. GraphRAGRetriever (Core Retrieval)
   - Implements hybrid search algorithm
   - Combines vector + graph approaches
   - Context ranking and formatting

3. EmbeddingManager (Vector Operations)
   - Handles OpenAI embedding generation
   - Manages vector storage and retrieval
   - Semantic similarity calculations

4. DataPersistenceManager (Caching)
   - Smart cache management
   - Incremental processing
   - Data validation and recovery

5. SequentialThinkingOrchestrator (Advanced Reasoning)
   - Multi-step query decomposition
   - Iterative reasoning process
   - Complex answer synthesis

WHY THIS APPROACH WORKS
=======================

1. Captures Explicit Relationships
- Traditional RAG misses explicit connections you've made
- System follows your own linking patterns
- Preserves intentional knowledge structure

2. Provides Richer Context
- Vector search: "Find similar topics"
- Graph traversal: "Include what I explicitly connected"
- Combined: More comprehensive context for the LLM

3. Scales Intelligently
- Small vaults: Fast processing, immediate insights
- Large vaults: Intelligent caching, incremental updates
- Complex queries: Sequential reasoning breaks down problems

TECHNICAL INNOVATION
====================

This implementation represents significant advancement over traditional RAG:

1. Hybrid Retrieval: Combines semantic + structural knowledge
2. Sequential Reasoning: Handles complex, multi-part queries  
3. Obsidian-Native: Leverages [[wikilinks]] as first-class graph edges
4. Performance Optimized: Intelligent caching and incremental processing
5. Multiple Interfaces: CLI, Web, and 3D visualization options

SYSTEM WORKFLOW SUMMARY
=======================

1. INITIALIZATION
   - Load configuration from .env file
   - Initialize OpenAI client
   - Set up logging and directories

2. DATA PROCESSING
   - Scan Obsidian vault for markdown files
   - Parse documents and extract metadata
   - Build knowledge graph from wikilinks
   - Generate vector embeddings
   - Cache all processed data

3. QUERY PROCESSING
   - Accept user query
   - Perform semantic search
   - Expand context via graph traversal
   - Format context for LLM
   - Generate response with citations

4. INTERFACE OPTIONS
   - Web Interface: Modern browser-based chat
   - CLI Interface: Terminal-based interaction
   - 3D Visualization: Interactive graph exploration

This creates a system that truly understands and leverages the knowledge 
structure built in Obsidian, rather than treating it as just a collection 
of text files.

===============================================================================
                              END OF DOCUMENTATION
=============================================================================== 